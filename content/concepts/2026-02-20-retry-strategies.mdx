---
title: "Retry Strategies"
slug: "2026-02-20-retry-strategies"
date: "2026-02-20"
category: "Reliability"
difficulty: "Intermediate"
tags: ["Retry", "Reliability", "Fault Tolerance", "Backoff"]
description: "Implementing smart retry logic with exponential backoff and jitter to handle transient failures gracefully"
---

## Overview

Transient failures are inevitable in distributed systems. A database might be briefly overloaded, a network request might time out, or a downstream service might return a 503. Retry strategies let your application recover from these temporary failures automatically, without human intervention.

The key is retrying smartly. Naive retries can make things worse by overwhelming an already struggling service.

## Exponential Backoff

Instead of retrying immediately, wait progressively longer between attempts.

```python
import time
import random

def retry_with_backoff(fn, max_retries=5, base_delay=1.0):
    for attempt in range(max_retries):
        try:
            return fn()
        except TransientError as e:
            if attempt == max_retries - 1:
                raise

            delay = base_delay * (2 ** attempt)  # 1s, 2s, 4s, 8s, 16s
            jitter = random.uniform(0, delay * 0.5)
            wait = delay + jitter

            print(f"Attempt {attempt + 1} failed: {e}. Retrying in {wait:.1f}s")
            time.sleep(wait)
```

## Why Jitter Matters

Without jitter, all clients retry at the exact same time, creating a "thundering herd" that crashes the recovering service.

```
Without jitter (thundering herd):
  Client A: retry at 1s, 2s, 4s
  Client B: retry at 1s, 2s, 4s  ← all hit the server at the same time
  Client C: retry at 1s, 2s, 4s

With jitter (spread out):
  Client A: retry at 1.3s, 2.7s, 4.1s
  Client B: retry at 0.8s, 1.9s, 3.5s  ← load is distributed
  Client C: retry at 1.1s, 2.4s, 5.2s
```

## What to Retry

**Retry:** Network timeouts, 502/503/504 errors, connection refused, database deadlocks

**Don't retry:** 400 Bad Request, 401 Unauthorized, 404 Not Found, 422 Validation Error -- these won't succeed on retry

```python
RETRYABLE_STATUS_CODES = {502, 503, 504, 429}
RETRYABLE_EXCEPTIONS = (ConnectionError, TimeoutError, IOError)

def should_retry(error):
    if isinstance(error, HTTPError):
        return error.status_code in RETRYABLE_STATUS_CODES
    return isinstance(error, RETRYABLE_EXCEPTIONS)
```

## Retry with Circuit Breaker

Combine retries with a circuit breaker to stop retrying when a service is clearly down.

```python
import httpx
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=1, max=10),
    retry=retry_if_exception(should_retry),
)
def call_payment_service(order):
    response = httpx.post(
        "http://payments-svc/charge",
        json={"amount": order.total},
        timeout=5.0,
    )
    response.raise_for_status()
    return response.json()
```

## Rate Limit Retries (429)

When a service returns `429 Too Many Requests`, it often includes a `Retry-After` header.

```python
def call_api_with_rate_limit(url):
    for attempt in range(5):
        response = httpx.get(url)
        if response.status_code == 429:
            retry_after = int(response.headers.get("Retry-After", 60))
            time.sleep(retry_after)
            continue
        response.raise_for_status()
        return response.json()
```

## Common Pitfalls

- **Retrying non-idempotent operations** -- Retrying a payment charge without idempotency keys can double-charge the customer.
- **No maximum retry limit** -- Infinite retries on a dead service waste resources. Always cap retries.
- **Retrying non-transient errors** -- A 400 Bad Request will fail every time. Only retry errors that might succeed.
- **Missing jitter** -- Synchronized retries amplify the problem they're trying to solve.

## Best Practices

- Use exponential backoff with jitter for all retries
- Set a maximum retry count (3-5 is typical)
- Only retry transient errors
- Respect `Retry-After` headers
- Make retried operations idempotent
- Combine retries with circuit breakers for downstream service calls
- Log every retry attempt for debugging
